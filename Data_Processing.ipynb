{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriavo256/cs123b-final-proj/blob/main/Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clinical Data Processing\n"
      ],
      "metadata": {
        "id": "izmuWKQEz92b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDZaMzpNb_9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0103d8-02d1-4b27-9d12-5350fe4743da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "PATH = \"/content/drive/Shareddrives/123B Project/279_cases/\"\n",
        "\n",
        "# Load the clinical.json file\n",
        "drive.mount('/content/drive')\n",
        "with open(PATH + 'clinical.json') as f:\n",
        "    samples = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dt_and_ps.csv (disease_type and primary_site)"
      ],
      "metadata": {
        "id": "ni504__QZy8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect data as a list of dictionaries\n",
        "data = []\n",
        "for sample in samples:\n",
        "    case_id = sample.get(\"case_id\", \"\")\n",
        "    row = {\"case_id\": case_id}\n",
        "    row[\"disease_type\"] = sample.get(\"disease_type\", \"\")\n",
        "    row[\"primary_site\"] = sample.get(\"primary_site\", \"\")\n",
        "    data.append(row)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "fieldnames = [\"case_id\", \"disease_type\", \"primary_site\"]\n",
        "df = pd.DataFrame(data, columns=fieldnames)\n",
        "df.to_csv(\"dt_and_ps.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/dt_and_ps.csv\", index=False)"
      ],
      "metadata": {
        "id": "lQY5bgOWZ-0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## exposure.csv"
      ],
      "metadata": {
        "id": "4H1hRBe50_oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all possible exposure keys\n",
        "exposure_keys = set()\n",
        "for sample in samples:\n",
        "    for exposure in sample.get(\"exposures\", []): # Some cases have multiple exposures\n",
        "        for key in exposure.keys():\n",
        "            exposure_keys.add(key)\n",
        "\n",
        "# Discard unnecessary keys\n",
        "discard_set = [\"tobacco_smoking_onset_year\", \"exposure_duration_years\", \"alcohol_history\",\n",
        "               \"type_of_tobacco_used\", \"created_datetime\", \"tobacco_smoking_quit_year\",\n",
        "               \"type_of_smoke_exposure\", \"exposure_id\", \"state\", \"submitter_id\",\n",
        "               \"secondhand_smoke_as_child\", \"updated_datetime\"]\n",
        "\n",
        "for item in discard_set:\n",
        "    exposure_keys.discard(item)\n",
        "\n",
        "print(exposure_keys)\n",
        "\n",
        "# Define DataFrame columns (including case_id)\n",
        "fieldnames = [\"case_id\"] + sorted(exposure_keys)\n",
        "\n",
        "print(fieldnames)\n",
        "\n",
        "# Collect data as a list of dictionaries\n",
        "data = []\n",
        "for sample in samples:\n",
        "    case_id = sample.get(\"case_id\", \"\")\n",
        "    row = {\"case_id\": case_id}\n",
        "    for exposure in sample.get(\"exposures\", []):\n",
        "        for key in exposure_keys:\n",
        "            if (key == \"years_smoked\" or\n",
        "                key == \"cigarettes_per_day\" or\n",
        "                key == \"pack_years_smoked\"):\n",
        "                value = exposure.get(key, 0)\n",
        "                row[key] = value\n",
        "            elif key == \"exposure_type\":\n",
        "                if exposure.get(\"exposure_type\"):\n",
        "                    row[key] = exposure.get(\"exposure_type\")\n",
        "                else:\n",
        "                    if exposure.get(\"tobacco_smoking_status\") != \"Lifelong Non-Smoker\":\n",
        "                        row[key] = \"Unknown\"\n",
        "                    else:\n",
        "                        row[key] = \"None\"\n",
        "            else:\n",
        "                row[key] = exposure.get(key)\n",
        "    data.append(row)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df = pd.DataFrame(data, columns=fieldnames)\n",
        "df.to_csv(\"exposures.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/exposures.csv\", index=False) # Save a copy to shared drive"
      ],
      "metadata": {
        "id": "bAiXpMLMihXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df814235-d7e8-4da5-b485-3d01a9be9c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cigarettes_per_day', 'years_smoked', 'exposure_type', 'pack_years_smoked', 'alcohol_intensity', 'tobacco_smoking_status'}\n",
            "['case_id', 'alcohol_intensity', 'cigarettes_per_day', 'exposure_type', 'pack_years_smoked', 'tobacco_smoking_status', 'years_smoked']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## demographic.csv"
      ],
      "metadata": {
        "id": "klS43uCP1Gr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a set with keys from the first sample\n",
        "common_demographic_keys = set(samples[0][\"demographic\"].keys())\n",
        "\n",
        "# Iterate through all the samples and perform intersection with their keys\n",
        "for sample in samples[1:]:\n",
        "    common_demographic_keys &= set(sample[\"demographic\"].keys())\n",
        "\n",
        "# Discard unnecessary keys\n",
        "discard_set = [\"created_datetime\", \"demographic_id\", \"age_is_obfuscated\",\n",
        "               \"year_of_birth\", \"state\", \"submitter_id\", \"updated_datetime\",\n",
        "               \"ethnicity\"]\n",
        "for item in discard_set:\n",
        "    common_demographic_keys.discard(item)\n",
        "\n",
        "# Collect data as a list of dictionaries\n",
        "data = []\n",
        "for sample in samples:\n",
        "    case_id = sample.get(\"case_id\", \"\")\n",
        "    row = {\"case_id\": case_id}\n",
        "    demographic = sample.get(\"demographic\", [])\n",
        "    for key in common_demographic_keys:\n",
        "        if key == \"days_to_birth\":\n",
        "            value = (-1 * demographic.get(key, 0)) // 365.25\n",
        "            row[\"age\"] = value\n",
        "        else:\n",
        "            value = demographic.get(key, \"\")\n",
        "            row[key] = value\n",
        "    data.append(row)\n",
        "\n",
        "# Define DataFrame columns, replaing days_to_birth with age\n",
        "common_demographic_keys.discard(\"days_to_birth\")\n",
        "common_demographic_keys.add(\"age\")\n",
        "fieldnames = [\"case_id\"] + sorted(common_demographic_keys)\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df = pd.DataFrame(data, columns=fieldnames)\n",
        "df.to_csv(\"demograhic.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/demographic.csv\", index=False) # Save a copy to shared drive"
      ],
      "metadata": {
        "id": "vScz322gwSt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## diagnoses.csv"
      ],
      "metadata": {
        "id": "MWKqtD751Lv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filters for the nested stuff\n",
        "freq = {}\n",
        "count = 0\n",
        "for sample in samples:\n",
        "    diagnoses = sample.get(\"diagnoses\", [])\n",
        "    for diag in diagnoses:\n",
        "        pathology_list = diag.get(\"pathology_details\", [])\n",
        "        if len(pathology_list) > 0:\n",
        "            count += 1\n",
        "print(\"Diagnoses with pathology details:\", count)\n",
        "\n",
        "# 162 has, 117 missing ~ 58%\n",
        "\n",
        "# remove = [\"updated_datetime\", \"created_datetime\", \"submitter_id\", \"treatment_id\"]\n",
        "# IDK the threshhold we wanna cut off on\n",
        "# updated = {}\n",
        "# for key in sorted_dict:\n",
        "#   if key in remove or sorted_dict[key] < 50:\n",
        "#     continue\n",
        "#   else:\n",
        "#     updated[key] = sorted_dict[key]\n",
        "\n",
        "# print(updated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdfcs2Du-rEB",
        "outputId": "7a225c5c-c0bc-4b43-f292-0d8e89c5a566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diagnoses with pathology details: 162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Discarded set - can determine later\n",
        "keys = set()\n",
        "discard = [\"submitter_id\", \"treatment_id\", \"diagnosis_id\", \"created_datetime\", \"updated_datetime\", \"pathology_details\", \"treatments\"]\n",
        "for sample in samples:\n",
        "    diagnoses = sample.get(\"diagnoses\", [])\n",
        "    for diagnosis in diagnoses:\n",
        "        for key in diagnosis.keys():\n",
        "          if key not in discard:\n",
        "            keys.add(key)\n",
        "\n",
        "        # just a list of dictionaries for the nested columns - still skeptical so will ask - also not filtered list, so can change later\n",
        "        if \"treatments\" in diagnosis:\n",
        "          keys.add(\"immunotherapy_treatments\")\n",
        "          keys.add(\"chemotherapy_treatments\")\n",
        "          keys.add(\"radiation_therapy_treatments\")\n",
        "          keys.add(\"treatment_outcome\")\n",
        "        if \"pathology_details\" in diagnosis:\n",
        "          keys.add(\"lymph_nodes_positive\")\n",
        "          keys.add(\"tumor_largest_dimension_diameter\")\n",
        "\n",
        "file_headers = [\"case_id\"] + sorted(list(keys))\n",
        "rows = []\n",
        "for sample in samples:\n",
        "  case_id = sample.get(\"case_id\")\n",
        "  diagnoses = sample.get(\"diagnoses\", [])\n",
        "\n",
        "  #If a case patient has more than one diagnosis, it will create another row for the SAME case ID (i.e. a case patient can have more than one row)\n",
        "  for diagnosis in diagnoses:\n",
        "    # and then consider if the header is nested lol\n",
        "    row = {\"case_id\": case_id}\n",
        "\n",
        "    #regular fields\n",
        "    for key in keys:\n",
        "      row[key] = diagnosis.get(key, None)\n",
        "\n",
        "    # Initialize treatment types columns with 0\n",
        "    row[\"immunotherapy_treatments\"] = 0\n",
        "    row[\"chemotherapy_treatments\"] = 0\n",
        "    row[\"radiation_therapy_treatments\"] = 0\n",
        "    row[\"treatment_outcome\"] = -1  # initialized as unknown\n",
        "\n",
        "    #fill treatments\n",
        "    if \"treatments\" in diagnosis:\n",
        "       # There are only 3 reported treatment types in the dataset:\n",
        "       #    Immunotherapy (Including Vaccines)\n",
        "       #    Chemotherapy\n",
        "       #    Radiation Therapy, NOS\n",
        "       # Store how many of each treatment the patient receives\n",
        "       for treatment in diagnosis.get(\"treatments\", []):\n",
        "          treatment_type = treatment.get(\"treatment_type\", \"\")\n",
        "          if treatment_type == \"Immunotherapy (Including Vaccines)\":\n",
        "              row[\"immunotherapy_treatments\"] += 1\n",
        "          elif treatment_type == \"Chemotherapy\":\n",
        "              row[\"chemotherapy_treatments\"] += 1\n",
        "          else:\n",
        "              row[\"radiation_therapy_treatments\"] += 1\n",
        "       # Also, store the treatment_outcome\n",
        "       #    Complete Response = 1\n",
        "       #    Persistent Disease = 0\n",
        "       #    Unknown = -1 (to be deleted later in treatment outcome model) --> turns out the final diagnoses.csv has none of these!\n",
        "       # NOTE: some cases had multiple outcomes, but the updated dates are all the same\n",
        "       # so instead of recording the most recent treatment outcome, I just recorded '1'\n",
        "       # if the patient had any complete responses recorded\n",
        "          treatment_outcome = treatment.get(\"treatment_outcome\", \"\")\n",
        "          if treatment_outcome == \"Complete Response\":\n",
        "              row[\"treatment_outcome\"] = 1\n",
        "          elif treatment_outcome == \"Persistent Disease\" and row[\"treatment_outcome\"] == -1:\n",
        "              row[\"treatment_outcome\"] = 0\n",
        "\n",
        "    #fill pathology details\n",
        "    if \"pathology_details\" in diagnosis:\n",
        "        pathology_detail = diagnosis.get(\"pathology_details\", [])\n",
        "        row[\"lymph_nodes_positive\"] = pathology_detail[0].get(\"lymph_nodes_positive\")\n",
        "        row[\"tumor_largest_dimension_diameter\"] = pathology_detail[0].get(\"tumor_largest_dimension_diameter\")\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=file_headers)\n",
        "df.to_csv(\"diagnoses.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/diagnoses.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "9oNiTNl9x-B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## follow_ups.csv"
      ],
      "metadata": {
        "id": "OBuvpzoGplzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the important keys/fields we will be using (you can delete unwanted keys here!)\n",
        "follow_ups_keys = [\"karnofsky_performance_status\", \"ecog_performance_status\", \"disease_response\", \"other_clinical_attributes\"]  # other_clinical_attributes contains bmi\n",
        "\n",
        "# Collect data\n",
        "data = []\n",
        "for sample in samples:\n",
        "    row = {\"case_id\": sample.get(\"case_id\", \"\")}\n",
        "    # Initialize last reported dates for the relevant fields\n",
        "    dr_last_reported = 0\n",
        "    prog_last_reported = 0\n",
        "    # Go through each follow up and store its data\n",
        "    for follow_up in sample.get(\"follow_ups\", []):\n",
        "        for key in follow_ups_keys:\n",
        "            # Update time (to be used for getting last reported fields)\n",
        "            time = follow_up.get(\"days_to_follow_up\", \"\")\n",
        "            try:\n",
        "                time = int(time)\n",
        "            except ValueError:\n",
        "                time = -1\n",
        "\n",
        "            match key:\n",
        "                case \"karnofsky_performance_status\":\n",
        "                    # Store minimum value (least healthy on scale of 0-100)\n",
        "                    karnofsky = follow_up.get(key, \"\")\n",
        "                    try:\n",
        "                        karnofsky = float(karnofsky)\n",
        "                        if row.get(key) is None or karnofsky < row[key]:\n",
        "                            row[key] = karnofsky\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    # Alternatively, we can modify this to store the % of scores < 70 (just a suggested threshold, can be changed)\n",
        "                case \"ecog_performance_status\":\n",
        "                    # Store highest value (least healthy on scale of 1-5)\n",
        "                    ecog = follow_up.get(key, \"\")\n",
        "                    try:\n",
        "                        ecog = float(ecog)\n",
        "                        if row.get(key) is None or ecog > row[key]:\n",
        "                            row[key] = ecog\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    # Alternatively, we can modify this to store the % of scores > 2 (just a suggested threshold, can be changed)\n",
        "                case \"disease_response\":\n",
        "                    # Store last reported response\n",
        "                    if time > dr_last_reported:\n",
        "                        row[key] = follow_up.get(key, \"\")\n",
        "                        dr_last_reported = time\n",
        "                case \"other_clinical_attributes\":\n",
        "                    # Extract and store bmi from other_clinical_attributes\n",
        "                    for attr in follow_up.get(key, []):\n",
        "                        bmi = attr.get(\"bmi\", \"\")\n",
        "                        if (bmi):\n",
        "                            row[\"bmi\"] = bmi\n",
        "    data.append(row)\n",
        "\n",
        "# Get full list of field names for DataFrame columns\n",
        "fieldnames = follow_ups_keys # [\"case_id\", \"karnofsky_performance_status\", \"ecog_performance_status\", \"disease_response\", \"bmi\"]\n",
        "fieldnames.insert(0, \"case_id\")\n",
        "fieldnames.remove(\"other_clinical_attributes\")\n",
        "fieldnames.append(\"bmi\")\n",
        "\n",
        "# Crate DataFrame and save to CSV\n",
        "df = pd.DataFrame(data, columns=fieldnames)\n",
        "df.to_csv(\"follow_ups.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/follow_ups.csv\", index=False) # Save a copy to shared drive"
      ],
      "metadata": {
        "id": "uBnKDUlVpqkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MERGE"
      ],
      "metadata": {
        "id": "RgWXWcVwgB6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo = pd.read_csv(PATH + \"csv/demographic.csv\")\n",
        "diagnosis = pd.read_csv(PATH + \"csv/diagnoses.csv\")\n",
        "dtps = pd.read_csv(PATH + \"csv/dt_and_ps.csv\")\n",
        "exposures = pd.read_csv(PATH + \"csv/exposures.csv\")\n",
        "follow = pd.read_csv(PATH + \"csv/follow_ups.csv\")\n",
        "merged_df_1 = pd.merge(demo, diagnosis, on='case_id', how='inner')\n",
        "merged_df_2 = pd.merge(merged_df_1, dtps, on='case_id', how='inner')\n",
        "merged_df_3 = pd.merge(merged_df_2, exposures, on='case_id', how='inner')\n",
        "merged_df_4 = pd.merge(merged_df_3, follow, on='case_id', how='inner')\n",
        "print(merged_df_4.head())\n",
        "merged_df_4.to_csv(\"merged.csv\", index=False)\n",
        "merged_df_4.to_csv(PATH + \"csv/merged.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGc7XdYZgHXw",
        "outputId": "6470ebc9-a2da-4c94-e7a5-8d2e2c6eaf51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                case_id   age  gender   race vital_status  \\\n",
            "0  003f4f85-3244-4132-8c9d-c29f09382269  60.0  female  white        Alive   \n",
            "1  01b5cff1-824f-4fd4-a149-7bcbacf8c4dc  54.0    male  asian        Alive   \n",
            "2  01db2fd5-6443-4ed8-8318-3ef4f9012450  55.0  female  white        Alive   \n",
            "3  024bde91-ea3e-4157-83c4-8482801b00dc  71.0  female  white        Alive   \n",
            "4  028061ef-a99c-42dd-b011-344036e5357f  60.0    male  asian        Alive   \n",
            "\n",
            "   age_at_diagnosis ajcc_clinical_m ajcc_pathologic_m ajcc_pathologic_n  \\\n",
            "0             22056              M0                M0                N0   \n",
            "1             20054              M0                MX                N0   \n",
            "2             20260              MX                M1                N0   \n",
            "3             26003              MX                MX                N0   \n",
            "4             22039              M0                MX                N0   \n",
            "\n",
            "  ajcc_pathologic_stage  ...     alcohol_intensity cigarettes_per_day  \\\n",
            "0             Stage IIB  ...    Occasional Drinker                0.0   \n",
            "1               Stage I  ...    Occasional Drinker               20.0   \n",
            "2              Stage IV  ...  Lifelong Non-Drinker                0.0   \n",
            "3               Stage I  ...  Lifelong Non-Drinker                0.0   \n",
            "4             Stage IIB  ...    Occasional Drinker                5.0   \n",
            "\n",
            "   exposure_type pack_years_smoked                tobacco_smoking_status  \\\n",
            "0        Unknown               0.0  Current Reformed Smoker for > 15 yrs   \n",
            "1        Tobacco              29.0                        Current Smoker   \n",
            "2            NaN               0.0                   Lifelong Non-Smoker   \n",
            "3            NaN               0.0                   Lifelong Non-Smoker   \n",
            "4        Tobacco               7.5                        Current Smoker   \n",
            "\n",
            "   years_smoked  karnofsky_performance_status  ecog_performance_status  \\\n",
            "0           0.0                          90.0                      1.0   \n",
            "1          36.0                          90.0                      0.0   \n",
            "2           0.0                           NaN                      NaN   \n",
            "3           0.0                         100.0                      0.0   \n",
            "4          36.0                          70.0                      2.0   \n",
            "\n",
            "       disease_response    bmi  \n",
            "0         TF-Tumor Free  26.67  \n",
            "1         TF-Tumor Free  22.72  \n",
            "2         WT-With Tumor  29.38  \n",
            "3  CR-Complete Response  32.78  \n",
            "4         TF-Tumor Free  20.55  \n",
            "\n",
            "[5 rows x 48 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNNimputer and SimpleImputer to fill in missing data"
      ],
      "metadata": {
        "id": "eMrCOCrthXb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer     # to fill in missing numerical data\n",
        "# documentation: https://scikit-learn.org/stable/modules/impute.html#knnimpute\n",
        "from sklearn.impute import SimpleImputer  # to fill in missing categorical data\n",
        "# documentation: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZDWkBQRuhXGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load merged data\n",
        "df = pd.read_csv(PATH + \"csv/merged.csv\")\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numerical_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Impute missing values in numerical data with KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
        "df[numerical_cols] = knn_imputer.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Impute missing values in categorical data with SimpleImputer --> Might want to look into whether we actually want this, but just added it for now! -vicky\n",
        "simp_imputer = SimpleImputer(strategy=\"most_frequent\") # using most frequent strategy for now...\n",
        "df[categorical_cols] = simp_imputer.fit_transform(df[categorical_cols])\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df.to_csv(\"knn_imputed.csv\", index=False)\n",
        "df.to_csv(PATH + \"csv/knn_imputed.csv\", index=False) # Save a copy to shared drive"
      ],
      "metadata": {
        "id": "ccB_oglDhpNm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}